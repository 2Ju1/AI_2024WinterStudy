{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMUM5qtk+82p3cRC3xuwRs8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transformer 구현하기"],"metadata":{"id":"sxultRqgFFRp"}},{"cell_type":"markdown","source":["## 필요한 라이브러리 import하기"],"metadata":{"id":"uWlBq_MqFVmT"}},{"cell_type":"code","source":["%%capture\n","!pip install gdown\n","!pip install transformers\n","!pip install sentencepiece # MarianTokenizer 불러올 때 필요\n","!pip install sacremoses # MarianMTModel 에서 불러올 때 warning 뜨는 것 방지\n","!pip install einops # 지리는 einops 쓰기 (Einstein operations)"],"metadata":{"id":"XmXVG_IWeJEE","executionInfo":{"status":"ok","timestamp":1705745451658,"user_tz":-540,"elapsed":39127,"user":{"displayName":"‎이주원(호크마교양대학 호크마교양대학)","userId":"04513129207991464839"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import torch\n","from torch import nn, optim\n","from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n","from transformers import MarianMTModel, MarianTokenizer\n","import pandas as pd\n","from tqdm import tqdm\n","import math\n","from einops import rearrange\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIuqWicbFfg1","executionInfo":{"status":"ok","timestamp":1705745483383,"user_tz":-540,"elapsed":24386,"user":{"displayName":"‎이주원(호크마교양대학 호크마교양대학)","userId":"04513129207991464839"}},"outputId":"76e7d1bd-6b50-420f-f02a-0424b555d939"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","cpu\n"]}]},{"cell_type":"markdown","source":["## 하이퍼 파라미터"],"metadata":{"id":"IUECclstD4I4"}},{"cell_type":"code","source":["n_layers=3\n","d_model=256\n","d_ff=512\n","n_heads=8\n","drop_o=0.1"],"metadata":{"id":"Tov2Uq6YD7uD","executionInfo":{"status":"ok","timestamp":1705745486388,"user_tz":-540,"elapsed":284,"user":{"displayName":"‎이주원(호크마교양대학 호크마교양대학)","userId":"04513129207991464839"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# 모델"],"metadata":{"id":"tPHVfNy1FPRk"}},{"cell_type":"markdown","source":["## Multi-Head Attention"],"metadata":{"id":"V__QvLD9FRXY"}},{"cell_type":"code","source":["class MHA(nn.Module):\n","  def __init__(self, d_model, n_heads):\n","    super().__init__()\n","    self.n_heads=n_heads\n","\n","    self.fc_q=nn.Linear(d_model,d_model)\n","    self.fc_k=nn.Linear(d_model, d_model)\n","    self.fc_v=nn.Linear(d_model, d_model)\n","\n","    self.fc_o=nn.Linear(d_model, d_model)\n","\n","    self.scale=torch.sqrt(torch.tensor(d_model/n_heads))\n","\n","  def forward(self, Q, K, V, mask=None):\n","    Q=self.fc_q(Q)\n","    K=self.fc_k(K)\n","    V=self.fc_v(V)\n","    # rearrange해서 헤드 수 드러내게 바꾸는 과정 필요\n","\n","    attention_score=Q @ K.transpose(-2,-1)/self.scale #마지막 두 차원을 전치시키라는 의미\n","\n","    #시계열에서는 마스킹이 인코더에서 필요한지 모르겠음\n","    if mask is not None:\n","      attention_score[mask]=-1e10\n","\n","    attention_weights=torch.softmax(attention_score, dim=-1)\n","\n","    attention=attention_weights @ V\n","\n","    # 헤드 합치는 과정 필요x=rearrange(attention, )\n","    x=attention\n","    x=self.fc_o(x) #토론하는 과정의 레이어가 필요\n","    return x, attention_weights\n","\n","class FeedForward(nn.Module):\n","  def __init__(self, d_model, d_ff, drop_p):\n","    super().__init__()\n","\n","    self.linear=nn.Sequential(nn.Linear(d_model, d_ff),\n","                              nn.ReLU(),\n","                              nn.Dropout(drop_p),\n","                              nn,Linear(d_ff,d_model))\n","  def forward(self, x):\n","    x=self.linear(x)\n","    return x\n","\n"],"metadata":{"id":"c-sp8z5vFWvI","executionInfo":{"status":"ok","timestamp":1705745489199,"user_tz":-540,"elapsed":293,"user":{"displayName":"‎이주원(호크마교양대학 호크마교양대학)","userId":"04513129207991464839"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"dvpQWGgsGvU-"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","  def __init__(self, d_model, d_ff, n_heads, drop_p):\n","    super.__init__()\n","\n","    self.self_atten=MHA(d_model, n_heads)\n","    self.self_atten_LN=nn.LayerNorm(d_model)\n","\n","    self.FF=FeedForward(d_model, d_ff, drop_p)\n","    self.FF_LN=nn.LayerNorm(d_model)\n","\n","    self.dropout=nn.Dropout(drop_p)\n","\n","  def forward(self, x, enc_mask):\n","    residual=self.self_atten(x,x,x,enc_mask)\n","    residual=self.dropout(residual)\n","    x=self.self_atten_LN(x+residual)\n","\n","    residual=self.FF(x)\n","    residual=self.dropout(residual)\n","    x=self.FF_LN(x+residual)\n","\n","    return x\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n","    super().__init__()\n","\n","    self.scale=torch.sqrt(torch.tensor(d_model))\n","    self.input_embedding=input_embedding\n","    self.pos_embedding=nn.Embedding(max_len, d_model)\n","\n","    self.dropout=nn.Dropout(drop_p)\n","\n","    self.layers=nn.ModuleList([EncoderLayer(d_model, d_ff, n_heads, drop_p) for _ in range(n_layers)])\n","\n","  def forward(self, src, mask, atten_map_save=False):\n","    #시계열인 경우 위치 임베딩 차원 고려 다시\n","    pos=torch.arange(src.shape[1]).repeat(src.shape[0],1).to(DEVICE)\n","\n","    x=self.scale*self.input_embedding(src)+self.pos_embedding(pos)\n","    x=self.dropout(x)\n","\n","    for layer in self.layers:\n","      x=layer(x,mask)\n","\n","    return x #하늘 정원 아웃풋\n","\n"],"metadata":{"id":"DogzyQ5KQhvo","executionInfo":{"status":"ok","timestamp":1705745491981,"user_tz":-540,"elapsed":2,"user":{"displayName":"‎이주원(호크마교양대학 호크마교양대학)","userId":"04513129207991464839"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Decoder"],"metadata":{"id":"dgHalyK7UDFM"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","  def __init__(self, d_model, d_ff, n_heads, drop_p):\n","    super().__init__()\n","\n","    self.self_atten=MHA(d_model, n_heads)\n","    self.self_atten_LN=nn.LayerNorm(d_model)\n","\n","    self.enc_de_atten=MHA(d_model,n_heads)\n","    self.enc_dec_atten_LN=nn.LayerNorm(d_model)\n","\n","    self.FF=FeedForward(d_model, n_heads)\n","    self.FF_LN=nn.LayerNorm(d_model)\n","\n","    self.dropout=nn.Dropout(drop_p)\n","\n","  def froward(self, x, enc_out, dec_mask, enc_dec_mask):\n","    residual=self.self_atten(x,x,x,dec_mask)\n","    residual=self.dropout(residual)\n","\n","    x=self.self_atten_LN(x+residual)\n","\n","    residual=self.enc_dec_atten(x, enc_out, enc_out, enc_dec_mask) #Q는 디코더로부터 K,V는 인코더로부터\n","    residual=self.dropout(residual)\n","\n","    x=self.enc_dec_atten_LN(x+residual)\n","\n","    residual=self.FF(x)\n","    residual=self.dropout(residual)\n","\n","    x=self.FF_LN(x+residual)\n","\n","    return x\n","\n","class Decoder(nn.Module):\n","  def __init__(self, input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p):\n","    super().__init__()\n","\n","    self.scale=torch.sqrt(torch.tensor(d_model))\n","    self.input_embedding=input_embedding\n","    self.pos_embedding=nn.Embedding(max_len, d_model)\n","\n","    self.dropout=nn.Dropout(drop_p)\n","\n","    self.layers=nn.ModuleLIst([DecoderLayer(d_model, d_FF, n_heads, drop_p) for _ in range(n_layers)])\n","\n","    self.fc_out=nn.Linear(d_model, vocab_size) #마지막에 원하는 것을 시계열로 바꿔주는 과정 필요\n","\n","  def forward(self, trg, enc_out, dec_mask, enc_dec_mask, atten_map_save=False):\n","\n","    pos=torch.arrange(trg.shape[1]).repeat(trg.shape[0],1).to(DEVICE) #위치 임베딩 시계열의 경우 주의해주기\n","\n","    x=self.scale*self.input_embedding(trg)+self.pos_embedding(pos)\n","    #self.scale을 곱해주면position보다 token 정보를 더 보게 된다\n","\n","    x=self.dropout(x)\n","\n","    for layer in self.layers:\n","      x=layer(s, enc_out, dec_mask, enc_dec_mask)\n","\n","    x=self.fc_out(x)\n","\n","    return x\n","\n"],"metadata":{"id":"9qpuyOP_Sy4y","executionInfo":{"status":"ok","timestamp":1705745538840,"user_tz":-540,"elapsed":3,"user":{"displayName":"‎이주원(호크마교양대학 호크마교양대학)","userId":"04513129207991464839"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## 모델"],"metadata":{"id":"j6odxIxnWRTl"}},{"cell_type":"code","source":["class Transformer(nn.Module):\n","  def __init__(self, vocab_size, max_len, ne_layers, d_model, d_ff, n_heads, drop_p):\n","    super.__init__()\n","\n","    self.input_embedding=nn.Embedding(vocab_size, d_model)\n","\n","    self.encoder=Encoder(self.input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p)\n","    self.decoder=Decoder(self.input_embedding, max_len, n_layers, d_model, d_ff, n_heads, drop_p)\n","\n","    self.n_heads=n_heads\n","\n","    for m in self.modules():\n","            if hasattr(m,'weight') and m.weight.dim() > 1: # 인풋 임베딩은 그대로 쓰기 위함\n","                nn.init.xavier_uniform_(m.weight) # xavier의 분산은 2/(Nin+Nout) 즉, 분산이 더 작다. => 그래서 sigmoid/tanh에 적합한 것! (vanishing gradient 막기 위해)\n","\n","  def make_enc_mask(self, src):\n","\n","    enc_mask=(src==pad_idx).unsqueeze(1).unsqueeze(2)  #시계열할 때 pad_idx로 통일시킬 필요 있음\n","    enc_mask=enc_mask.repeat(1, self.n_heads, src.shape[1],1)\n","\n","    return enc_mask\n","\n","  def make_dec_maks(self, trg):\n","    trg_pad_mask=(trg.to('cpu')==pad_idx).unsqueeze(1).unsqueeze(2)\n","    trg_pad_maks=trg_pad_mask.repeat(1, self.n_jeads, trg.shape[1],1)\n","\n","    trg_future_mask=torch.trill(torch.ones(trg.shape[0],self.n_heads, trg.shape[1],trg.shape[1]))==0\n","\n","    dec_mask=trg_pad_mask | trg_future_mask\n","\n","    return dec_mask\n","\n","  def make_enc_dec_mask(self, src, trg):\n","    enc_dec_mask=(src==pad_idx).unsqueeze(1).unsqueeze(2)\n","    enc_dec_maks=enc_dec_mask.repeat(1, self.n_heads, trg.shape[1],1)\n","\n","    return enc_dec_mask\n","\n","  def forward(self, src, trg):\n","\n","    enc_mask=self.make_enc_mask(src)\n","    dec_mask=self.make_dec_mask(src)\n","    enc_dec_mask=self. make_enc_dec_mask(src, trg)\n","\n","    enc_out=self.encoder(src, enc_mask)\n","    out=self.decoder(trg, enc_out, dec_mask, enc_dec_mask)\n","\n","    return out\n","\n","\n","\n","\n"],"metadata":{"id":"WvEMHINqW62x"},"execution_count":null,"outputs":[]}]}